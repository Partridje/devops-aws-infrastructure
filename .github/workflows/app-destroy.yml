name: Destroy Application

# Application destroy is MANUAL ONLY
# This workflow stops the application without destroying infrastructure
# Infrastructure can be destroyed separately using terraform-destroy workflow
on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to destroy application from'
        required: true
        type: choice
        options:
          - dev
          - prod
      cleanup_images:
        description: 'Clean up old ECR images (keep last 5)'
        required: false
        type: boolean
        default: false
      reset_ssm:
        description: 'Reset SSM parameter to "initial"'
        required: false
        type: boolean
        default: true

permissions:
  id-token: write  # Required for OIDC
  contents: read

env:
  AWS_REGION: eu-north-1
  PROJECT_NAME: demo-app

jobs:
  # First, verify infrastructure exists
  verify-infrastructure:
    name: Verify Infrastructure
    runs-on: ubuntu-latest
    outputs:
      ecr_repository: ${{ steps.check-infra.outputs.ecr_repository }}
      asg_name: ${{ steps.check-infra.outputs.asg_name }}

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check infrastructure exists
        id: check-infra
        run: |
          ENV="${{ github.event.inputs.environment }}"

          echo "Checking infrastructure for environment: ${ENV}"

          # Check ECR Repository
          ECR_REPO="${{ env.PROJECT_NAME }}-${ENV}-app"
          if aws ecr describe-repositories --repository-names "${ECR_REPO}" --region ${{ env.AWS_REGION }} > /dev/null 2>&1; then
            echo "✓ ECR repository exists: ${ECR_REPO}"
            ECR_URI=$(aws ecr describe-repositories --repository-names "${ECR_REPO}" --region ${{ env.AWS_REGION }} --query 'repositories[0].repositoryUri' --output text)
            echo "ecr_repository=${ECR_URI}" >> $GITHUB_OUTPUT
          else
            echo "::warning::ECR repository not found: ${ECR_REPO}"
          fi

          # Check Auto Scaling Group
          ASG_NAME=$(aws autoscaling describe-auto-scaling-groups \
            --query "AutoScalingGroups[?starts_with(AutoScalingGroupName, '${{ env.PROJECT_NAME }}-${ENV}-')].AutoScalingGroupName" \
            --output text | head -n1)

          if [ -z "${ASG_NAME}" ]; then
            echo "::warning::Auto Scaling Group not found for environment: ${ENV}"
            echo "asg_name=" >> $GITHUB_OUTPUT
          else
            echo "✓ Auto Scaling Group exists: ${ASG_NAME}"
            echo "asg_name=${ASG_NAME}" >> $GITHUB_OUTPUT
          fi

          echo ""
          echo "Infrastructure verification completed"

  destroy:
    name: Destroy Application from ${{ github.event.inputs.environment }}
    needs: verify-infrastructure
    runs-on: ubuntu-latest
    environment:
      name: ${{ github.event.inputs.environment }}

    steps:
      #####################################
      # Checkout Code
      #####################################
      - name: Checkout repository
        uses: actions/checkout@v5

      #####################################
      # Configure AWS Credentials
      #####################################
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      #####################################
      # Stop Application on EC2 Instances
      #####################################
      - name: Stop application containers on all instances
        if: needs.verify-infrastructure.outputs.asg_name != ''
        run: |
          ASG_NAME="${{ needs.verify-infrastructure.outputs.asg_name }}"
          ENV="${{ github.event.inputs.environment }}"

          echo "Stopping application containers on all EC2 instances..."

          # Get list of instance IDs from ASG
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "${ASG_NAME}" \
            --query "AutoScalingGroups[0].Instances[?HealthStatus=='Healthy' && LifecycleState=='InService'].InstanceId" \
            --output text)

          if [ -z "$INSTANCE_IDS" ]; then
            echo "::warning::No healthy instances found in ASG"
            exit 0
          fi

          echo "Found instances: $INSTANCE_IDS"

          # Stop and remove Docker containers on all instances
          COMMAND_ID=$(aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --instance-ids $INSTANCE_IDS \
            --parameters 'commands=["if docker ps -a --format '\''{{.Names}}'\'' | grep -q '\''^application$'\''; then echo '\''Stopping application container...'\''; docker stop application || true; docker rm application || true; echo '\''✓ Application container stopped and removed'\''; else echo '\''No application container found'\''; fi"]' \
            --timeout-seconds 300 \
            --region ${{ env.AWS_REGION }} \
            --query "Command.CommandId" \
            --output text)

          echo "SSM Run Command initiated: ${COMMAND_ID}"
          echo "Waiting for containers to stop..."

          # Wait for command to complete on all instances
          for instance in $INSTANCE_IDS; do
            aws ssm wait command-executed \
              --command-id "${COMMAND_ID}" \
              --instance-id "$instance" \
              --region ${{ env.AWS_REGION }} \
              || echo "::warning::Instance $instance may have failed. Check SSM command output."
          done

          echo "✓ Application containers stopped on all instances"

      #####################################
      # Disable app-launcher systemd service
      #####################################
      - name: Disable app-launcher service
        if: needs.verify-infrastructure.outputs.asg_name != ''
        run: |
          ASG_NAME="${{ needs.verify-infrastructure.outputs.asg_name }}"

          echo "Disabling app-launcher systemd service on all instances..."

          # Get list of instance IDs from ASG
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "${ASG_NAME}" \
            --query "AutoScalingGroups[0].Instances[?HealthStatus=='Healthy' && LifecycleState=='InService'].InstanceId" \
            --output text)

          if [ -z "$INSTANCE_IDS" ]; then
            echo "::warning::No healthy instances found"
            exit 0
          fi

          # Disable systemd service
          COMMAND_ID=$(aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --instance-ids $INSTANCE_IDS \
            --parameters 'commands=["if systemctl is-enabled app-launcher.service > /dev/null 2>&1; then echo '\''Disabling app-launcher service...'\''; systemctl stop app-launcher.service || true; systemctl disable app-launcher.service || true; echo '\''✓ app-launcher service disabled'\''; else echo '\''app-launcher service not found or already disabled'\''; fi"]' \
            --timeout-seconds 120 \
            --region ${{ env.AWS_REGION }} \
            --query "Command.CommandId" \
            --output text)

          echo "SSM Run Command initiated: ${COMMAND_ID}"

          # Wait for command to complete on all instances
          for instance in $INSTANCE_IDS; do
            aws ssm wait command-executed \
              --command-id "${COMMAND_ID}" \
              --instance-id "$instance" \
              --region ${{ env.AWS_REGION }} \
              || echo "::warning::Instance $instance may have failed"
          done

          echo "✓ app-launcher service disabled on all instances"

      #####################################
      # Reset SSM Parameter
      #####################################
      - name: Reset SSM Parameter to initial
        if: github.event.inputs.reset_ssm == 'true'
        run: |
          ENV="${{ github.event.inputs.environment }}"
          SSM_PARAM="/${{ env.PROJECT_NAME }}/${ENV}/app/version"

          echo "Resetting SSM Parameter: ${SSM_PARAM} to 'initial'"

          aws ssm put-parameter \
            --name "${SSM_PARAM}" \
            --value "initial" \
            --type String \
            --overwrite \
            --region ${{ env.AWS_REGION }}

          echo "✓ SSM Parameter reset to 'initial'"

      #####################################
      # Clean up old ECR images
      #####################################
      - name: Clean up old ECR images
        if: github.event.inputs.cleanup_images == 'true' && needs.verify-infrastructure.outputs.ecr_repository != ''
        run: |
          ENV="${{ github.event.inputs.environment }}"
          ECR_REPO="${{ env.PROJECT_NAME }}-${ENV}-app"
          KEEP_COUNT=5

          echo "Cleaning up old ECR images, keeping last ${KEEP_COUNT}..."

          # Get all image digests sorted by push time (oldest first)
          IMAGE_DIGESTS=$(aws ecr list-images \
            --repository-name "${ECR_REPO}" \
            --region ${{ env.AWS_REGION }} \
            --query 'sort_by(imageDetails, &imagePushedAt)[*].imageDigest' \
            --output text)

          # Count total images
          TOTAL_IMAGES=$(echo "${IMAGE_DIGESTS}" | wc -w)
          echo "Total images in repository: ${TOTAL_IMAGES}"

          if [ ${TOTAL_IMAGES} -le ${KEEP_COUNT} ]; then
            echo "✓ No cleanup needed (${TOTAL_IMAGES} images <= ${KEEP_COUNT})"
            exit 0
          fi

          # Calculate how many to delete
          DELETE_COUNT=$((TOTAL_IMAGES - KEEP_COUNT))
          echo "Deleting ${DELETE_COUNT} old images..."

          # Delete old images (keep last KEEP_COUNT)
          echo "${IMAGE_DIGESTS}" | head -n ${DELETE_COUNT} | while read digest; do
            if [ -n "${digest}" ]; then
              echo "Deleting image: ${digest}"
              aws ecr batch-delete-image \
                --repository-name "${ECR_REPO}" \
                --region ${{ env.AWS_REGION }} \
                --image-ids imageDigest=${digest} || true
            fi
          done

          echo "✓ Cleaned up ${DELETE_COUNT} old images, kept last ${KEEP_COUNT}"

      #####################################
      # Verification
      #####################################
      - name: Verify application stopped
        if: needs.verify-infrastructure.outputs.asg_name != ''
        run: |
          ASG_NAME="${{ needs.verify-infrastructure.outputs.asg_name }}"

          echo "Verifying application is stopped..."

          # Get list of instance IDs
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names "${ASG_NAME}" \
            --query "AutoScalingGroups[0].Instances[?HealthStatus=='Healthy' && LifecycleState=='InService'].InstanceId" \
            --output text)

          if [ -z "$INSTANCE_IDS" ]; then
            echo "No instances to verify"
            exit 0
          fi

          # Check if containers are stopped
          COMMAND_ID=$(aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --instance-ids $INSTANCE_IDS \
            --parameters 'commands=["if docker ps --format '\''{{.Names}}'\'' | grep -q '\''^application$'\''; then echo '\''WARNING: Application container still running!'\''; exit 1; else echo '\''✓ Application container not running'\''; fi"]' \
            --timeout-seconds 60 \
            --region ${{ env.AWS_REGION }} \
            --query "Command.CommandId" \
            --output text)

          # Wait and check results
          sleep 5

          # Get command results
          for INSTANCE_ID in $INSTANCE_IDS; do
            STATUS=$(aws ssm get-command-invocation \
              --command-id "${COMMAND_ID}" \
              --instance-id "${INSTANCE_ID}" \
              --query "Status" \
              --output text 2>/dev/null || echo "Unknown")

            OUTPUT=$(aws ssm get-command-invocation \
              --command-id "${COMMAND_ID}" \
              --instance-id "${INSTANCE_ID}" \
              --query "StandardOutputContent" \
              --output text 2>/dev/null || echo "")

            echo "Instance ${INSTANCE_ID}: ${STATUS}"
            echo "${OUTPUT}"
          done

          echo "✓ Verification completed"

      #####################################
      # Summary
      #####################################
      - name: Destroy summary
        if: always()
        run: |
          echo "=========================================="
          echo "Application Destroy Summary"
          echo "=========================================="
          echo "Environment: ${{ github.event.inputs.environment }}"
          echo "ASG: ${{ needs.verify-infrastructure.outputs.asg_name }}"
          echo "Reset SSM: ${{ github.event.inputs.reset_ssm }}"
          echo "Cleanup ECR: ${{ github.event.inputs.cleanup_images }}"
          echo "=========================================="
          echo ""
          echo "✓ Application stopped on all instances"
          echo "✓ app-launcher service disabled"
          if [ "${{ github.event.inputs.reset_ssm }}" = "true" ]; then
            echo "✓ SSM parameter reset to 'initial'"
          fi
          if [ "${{ github.event.inputs.cleanup_images }}" = "true" ]; then
            echo "✓ Old ECR images cleaned up"
          fi
          echo ""
          echo "Infrastructure remains intact."
          echo "To destroy infrastructure, run 'terraform-destroy' workflow."
          echo ""
          echo "To redeploy application:"
          echo "  1. Run 'app-deploy' workflow"
          echo "  2. Or manually start containers on instances"
